## 4.8 扩展阅读

本章的大部分内容建立了基础数学，并将它们与研究映射的方法联系起来，其中许多方法是机器学习在支撑软件解决方案和几乎所有机器学习理论构建块层面上的核心。使用行列式、特征谱和特征空间对矩阵进行表征，为矩阵的分类和分析提供了基本特征和条件。这扩展到数据和涉及数据的映射的所有形式的表示，以及评估在这些矩阵上进行的计算操作的数值稳定性（Press et al.，2007）。

行列式是反转矩阵和“手动”计算特征值的基本工具。然而，对于几乎所有但不是最小的实例，通过高斯消元法进行的数值计算都优于行列式（Press et al.，2007）。尽管如此，行列式仍然是一个强大的理论概念，例如，可以根据行列式的符号直观地了解基的方向。特征向量可用于执行基变换，将数据转换为有意义的正交特征向量的坐标。同样，当我们计算或模拟随机事件时，矩阵分解方法（如楚列斯基分解）经常再次出现（Rubinstein和Kroese，2016）。因此，楚列斯基分解使我们能够计算重参数化技巧，其中我们希望在随机变量上进行连续微分，例如在变分自编码器（Jimenez Rezende et al.，2014；Kingma和Welling，2014）中。

特征分解对于使我们能够提取表征线性映射的有意义和可解释的信息至关重要。

因此，特征分解构成了一类称为谱方法的机器学习算法的基础，这类算法对正定核进行特征分解。这些谱分解方法涵盖了统计数据分析的经典方法，例如：

- 主成分分析（PCA）（Pearson, 1901，也见第10章），它寻求一个低维子空间，该子空间能解释数据中的大部分变异性。
- 费舍尔判别分析（Fisher discriminant analysis），旨在确定用于数据分类的分离超平面（Mika et al.，1999）。
- 多维标度（MDS）（Carroll和Chang，1970）。

这些方法的计算效率通常来源于找到对称正半定矩阵的最佳k秩近似。谱方法的更现代例子有不同的起源，但每个例子都需要计算正定核的特征向量和特征值，如Isomap（Tenenbaum et al.，2000）、拉普拉斯特征映射（Laplacian eigenmaps）（Belkin和Niyogi，2003）、海森特征映射（Hessian eigenmaps）（Donoho和Grimes，2003）和谱聚类（Shi和Malik，2000）。这些算法的核心计算通常基于低秩矩阵近似技术（Belabbas和Wolfe，2009），正如我们在这里通过奇异值分解（SVD）所遇到的那样。

SVD允许我们发现与特征分解相同类型的一些信息。然而，SVD更普遍地适用于非方阵和数据表。当我们想要通过近似进行数据压缩时（例如，不存储$n\times m$个值，而只存储$(n+m)k$个值），或者当我们想要进行数据预处理（例如，去相关设计矩阵的预测变量）（Ormoneit et al.，2001）时，这些矩阵分解方法变得相关。SVD作用于矩阵，我们可以将其解释为具有两个索引（行和列）的矩形数组。将类似矩阵的结构扩展到更高维度的数组称为张量。事实证明，SVD是作用于此类张量的更一般分解族的一个特例（Kolda和Bader，2009）。在张量上进行的类似SVD的操作和低秩近似，例如，有Tucker分解（Tucker，1966）或CP分解（Carroll和Chang，1970）。

出于计算效率的原因，SVD低秩近似在机器学习中经常被使用。这是因为它减少了我们可能需要在非常大的数据矩阵上执行的非零乘法操作的内存量和操作量（Trefethen和Bau III，1997）。此外，低秩近似还用于处理可能包含缺失值的矩阵，以及用于有损压缩和降维（Moonen和De Moor，1995；Markovsky，2011）。





