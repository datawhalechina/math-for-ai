<center><big><font color="red">本教程由 <a href="https://github.com/datawhalechina/math-for-ai">Datawhale 开源社区</a>编译，与对应的英文原版均开源免费</font></big></center>

## 12.6 拓展阅读

支持向量机（SVM）是研究二分类问题的众多方法之一。其他方法包括感知机、逻辑回归、费舍尔判别分析、最近邻、朴素贝叶斯和随机森林（Bishop, 2006; Murphy, 2012）。Ben-Hur等人（2008）的文献中提供了关于SVM和离散序列上核的简短教程。SVM的发展与第8.2节中讨论的经验风险最小化密切相关，因此SVM具有强大的理论特性（Vapnik, 2000; Steinwart和Christmann, 2008）。关于核方法的书籍（Schölkopf和Smola, 2002）详细介绍了支持向量机的许多细节以及如何优化它们。另一本关于核方法的更广泛的书籍（Shawe-Taylor和Cristianini, 2004）也包含了许多针对不同机器学习问题的线性代数方法。

利用勒让德-芬切尔变换（Legendre-Fenchel transform，第7.3.3节）的思想，可以得到对偶SVM的另一种推导。该推导分别考虑了SVM无约束形式（12.31）的每一项，并计算了它们的凸共轭（Rifkin和Lippert, 2007）。对SVM的功能分析视角（也是正则化方法视角）感兴趣的读者可以参考Wahba（1990）的工作。核的理论阐述（Aronszajn, 1950; Schwartz, 1964; Saitoh, 1988; Manton和Amblard, 2015）需要线性算子基础知识（Akhiezer和Glazman, 1993）。核的概念已被推广到巴拿赫空间（Banach spaces）（Zhang等人, 2009）和克列因空间（Kreǐn spaces）（Ong等人, 2004; Loosli等人, 2016）。

请注意，合页损失函数有三种等价表示，如（12.28）和（12.29）所示，以及（12.33）中的约束优化问题。在将SVM损失函数与其他损失函数进行比较时，（12.28）式经常被使用（Steinwart, 2007）。（12.29）式的两段形式便于计算次梯度，因为每段都是线性的。第12.5节中看到的第三种形式（12.33）使得能够使用凸二次规划（第7.3.2节）工具。

由于二分类是机器学习中研究得很好的一项任务，因此有时也会使用其他术语，如判别、分离和决策。此外，二分类器的输出可以是三个量之一。首先是线性函数本身的输出（通常称为分数），它可以取任何实数值。这个输出可以用于对示例进行排名，而二分类可以被认为是在排名后的示例上选择一个阈值（Shawe-Taylor和Cristianini, 2004）。第二个经常被认为是二分类器输出的是，在通过非线性函数传递后确定的输出，以将其值限制在有界范围内，例如在区间[0,1]内。一个常见的非线性函数是Sigmoid函数（Bishop, 2006）。当非线性结果得到良好校准的概率（Gneiting和Raftery, 2007; Reid和Williamson, 2011）时，这被称为类别概率估计。二分类器的第三个输出是最终的二值决策{+1,-1}，这是最常假设为分类器输出的形式。

SVM是一种二分类器，它本身并不自然地适合概率解释。有几种方法可以将线性函数的原始输出（分数）转换为校准后的类别概率估计（$P(Y=1|X=x)$），这些方法涉及一个额外的校准步骤（Platt, 2000; Zadrozny和Elkan, 2001; Lin等人, 2007）。从训练的角度来看，有许多相关的概率方法。在第12.2.5节的末尾，我们提到损失函数和似然之间存在关系（也请比较第8.2节和第8.3节）。在训练过程中，与良好校准的变换相对应的最大似然方法称为逻辑回归，它来自一类称为广义线性模型的方法。从这个角度来看，逻辑回归的详细信息可以在Agresti（2002，第5章）和McCullagh和Nelder（1989，第4章）中找到。当然，可以通过使用贝叶斯逻辑回归估计后验分布来更贝叶斯地看待分类器输出。贝叶斯视角还包括先验的规范，其中包括与似然相关的设计选择，如共轭性（第6.6.1节）。此外，还可以将潜在函数视为先验，这导致了高斯过程的分类(Rasmussen and Williams, 2006, chapter 3).
