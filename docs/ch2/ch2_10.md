
# 番外篇 多重线性代数与张量

> 本文主要参照 Youtube 博主 [@eigenchris](https://www.youtube.com/@eigenchris) 的张量简介视频。
> * 原链接：[https://www.youtube.com/watch?v=8ptMTLzV4-I](https://www.youtube.com/watch?v=8ptMTLzV4-I)
> * b站搬运链接：[https://www.bilibili.com/video/BV1cr4y147eW?](https://www.bilibili.com/video/BV1cr4y147eW?)

# 0 张量就是多维数组吗？
## 0.1 张量的三个理解

### 0.1.1 数组定义：张量是多维数组

> Tensors are multi-dimensional arrays
> 张量是多维数组

| 常用名称     | 张量阶数 |
| -------- | :--: |
| 数        | $0$  |
| 向量       | $1$  |
| 矩阵       | $2$  |
| 三维及以上的数组 | $3+$ |

* 这是一个错误的定义，**数组只是张量的表示**，而不是张量本身

### 0.1.2 坐标定义：张量是坐标变换下的不变量

> Tensors are objects that are **invarint** under a change of coordinates, and has components that change in a special, predictable way under a change of coordinates
>
> 张量是在坐标变换变换下**保持不变**的数学对象，其分量在坐标变换时以一种特殊且可预测的方式变化

<center>
<img src="./ch2/attachments/inv_pencil.png" alt="坐标是变化的，笔是不变的" style="zoom:80%;">
</center>
<center>图1 坐标是变化的，笔是不变的</center>

* 拿起你手中的笔，将其指向你房间的门或其他地方。不论你将你手中的笔指向哪里，它的形状、大小等性质都不会改变。
* 想象你现在建立了一个三维坐标系（虽然这听起来很蠢），这支笔可以写成基向量的线性组合。当你建立另一个坐标系时，这支笔在这个坐标系下的坐标会发生变化。而笔本身并没有改变。
* 不同坐标系下笔对应的坐标是变化的，但笔是不变的——**向量在不同坐标系下的表示是变化的，而向量本身是不变的**

### 0.1.3 张量的抽象定义

> Tensor is a collection of vectors and covectors combined together using the **tensor product**
>
> 张量是一组使用**张量积**组合在一起的向量和余向量

读者在此可能会遇见问题：这个定义确实简洁，我听过“向量”但我不知道什么是“余向量”，也不知道什么是“张量积”。此定义我们先按下不表，到最后想必读者在明白一切后会明白这个定义的绝妙之处。

# 1 重回线性代数
## 1.1 基变换

我们举二维Euclid空间（平面）$\mathbb{R}^{2}$作为例子。我们取平面上的两个基：$\{ \boldsymbol{e}_{1}, \boldsymbol{e}_{2} \}$ 和 $\{ \tilde{\boldsymbol{e}}_{1}, \tilde{\boldsymbol{e}}_{2} \}$。我们在此做一个约定，从 $\{ \boldsymbol{e}_{1}, \boldsymbol{e}_{2} \}$ 变换到 $\{ \tilde{\boldsymbol{e}}_{1}, \tilde{\boldsymbol{e}}_{2} \}$ 我们称为**正向变换（forward transformation）**，反之就称为 **逆向变换（backward transformation）**。按照线性空间性质，后者的每个基向量一定可以写成前面的基向量的线性组合：

$$
\begin{aligned}
\tilde{\boldsymbol{e}}_{1} &= f_{1,1} \boldsymbol{e}_1 + f_{2,1} \boldsymbol{e}_{2},\\
\tilde{\boldsymbol{e}}_{2} &= f_{1,2} \boldsymbol{e}_1 + f_{2,2} \boldsymbol{e}_{2};\\
\end{aligned} \tag{1}
$$

因此正向变换矩阵就可以写为

$$
\boldsymbol{F} = \begin{bmatrix}
f_{1,1} & f_{1,2}\\f_{2,1} & f_{2,2}
\end{bmatrix}. \tag{2}
$$

反过来， $\{ \boldsymbol{e}_{1}, \boldsymbol{e}_{2} \}$ 也可以由 $\{ \tilde{\boldsymbol{e}}_{1}, \tilde{\boldsymbol{e}}_{2} \}$ 的线性组合表示出来

$$
\begin{aligned}
\boldsymbol{e}_{1} &= b_{1,1} \tilde{\boldsymbol{e}}_1 + b_{2,1} \tilde{\boldsymbol{e}}_{2},\\
\boldsymbol{e}_{2} &= b_{1,2} \tilde{\boldsymbol{e}}_1 + b_{2,2} \tilde{\boldsymbol{e}}_{2};\\
\end{aligned}\tag{3}
$$

因此得到后向变换矩阵：

$$
\boldsymbol{B} = \begin{bmatrix}
b_{1,1} & b_{1, 2}\\ b_{2, 1} & b_{2, 2}
\end{bmatrix}. \tag{4}
$$

现在我们可以做一个小小的变换：将 (1) 代入 (3)。对于 $\boldsymbol{e}_{1}$，我们有

$$
\begin{align}
\boldsymbol{e}_{1} &= b_{1, 1} \tilde{\boldsymbol{e}}_{1} + b_{2,1}\tilde{\boldsymbol{e}}_{2} & \text{按照(3)的写法}\\
&= b_{1, 1} \Big( f_{1,1} \boldsymbol{e}_1 + f_{2,1} \boldsymbol{e}_{2} \Big)  + b_{2,1} \Big( f_{2,1} \boldsymbol{e}_1 + f_{2,2} \boldsymbol{e}_{2} \Big) &\text{将(1)代入(3)}\\
&=  \Big(f_{1,1}b_{1, 1}   + f_{1,2} b_{2,1}  \Big) \boldsymbol{e}_1 +  \Big(f_{2,1}b_{1, 1}   + f_{2,2}b_{2,1} \Big)\boldsymbol{e}_{2} &\text{将(1)代入(3)}\\
\end{align} \tag{5}
$$

此时注意到系数对应相等，就得到了

$$
\begin{align}
f_{1,1}b_{1, 1}   + f_{1,2} b_{2,1} = 1,\\
f_{2,1}b_{1, 1}   + f_{2,2}b_{2,1} = 0.\\
\end{align} \tag{6}
$$

这样的情形对于 $\boldsymbol{e}_{2}$ 也是类似的。因此我们可以得到正向变换矩阵是反向变换矩阵的逆：

$$
F = B^{-1}.\tag{7}
$$

但为了作区分，下文中依然将使用 $F$ 和 $B$ 的记号。

再熟悉二维情形后，我们扩展至 $n$ 维的情形。$\mathbb{R}^{n}$ 中有两个基 $\{ \boldsymbol{e}_{i} \}_{i=1}^{n}$ 和 $\{ \tilde{\boldsymbol{e}}_{i} \}_{i=1}^{n}$，它们可以互相表出：

$$
\boxed{\begin{align}
{\color{red} \boldsymbol{e}_{i} }  &= \sum\limits_{j=1}^{n} b_{j, i} {\color{blue} \tilde{\boldsymbol{e}}_{j} } \tag{8a}\\
{\color{blue} \tilde{\boldsymbol{e}}_{j} }  &= \sum\limits_{k=1}^{n} f_{k, j} {\color{red} \boldsymbol{e}_{k} }\tag{8b}
\end{align}}
$$

然后我们再做和 (5) 相同的事情，即将 (8b) 代入 (8a)：

$$
\begin{align}
{\color{red} \boldsymbol{e}_{i} }  &= \sum\limits_{j=1}^{n} b_{j, i} {\color{blue} \tilde{\boldsymbol{e}}_{j} } \\
 &= \sum\limits_{j=1}^{n} b_{j, i} \Big( \sum\limits_{k=1}^{n} f_{k, j} {\color{red} \boldsymbol{e}_{k} } \Big)  \\
 &= \sum\limits_{j=1}^{n}  \Big( \sum\limits_{k=1}^{n} f_{k, j}b_{j, i} \Big) {\color{red} \boldsymbol{e}_{k} } \\
\end{align}\tag{9}
$$

因此我们有

$$
\boxed{\sum\limits_{j=1}^{n}  \Big( \sum\limits_{k=1}^{n} f_{k, j}b_{j, i} \Big) = \delta_{j,k} = \begin{cases}
1, & j = k;\\ 0, & j \neq k.
\end{cases} \tag{10}}
$$

这也再次证明了正向变换和反向变换互为逆变换。(10) 中的 $\delta_{j,k}$ 被称为 Kronecker-delta 记号。

## 1.2 线性空间

对于向量的理解，也有三种
1. 向量是一个数为元素的列表
2. 向量是像箭头一样的东西
3. 向量是**线性空间**中的元素

那么线性空间是什么？我想有过线性代数背景的读者都会知道。我们在此快速引入一些更加抽象的概念，然后自然地过渡到线性空间。它们在将来的内容中也会派上用场。

### 1.2.1 群、环、域

**群（group）** 指的是一组资料 $(G, \cdot)$ 其中 $G$ 是一个非空集合，$\cdot: G \times G \rightarrow G$ 是 $G$ 上的一个二元运算（通常称为乘法）。它们满足下面的条件
1. （结合律）对于任意 $a, b, c \in G$，都有 $a \cdot (b \cdot c) = (a \cdot b) \cdot c$
2. （中性元）存在 $1_{G}$，使得对任意 $g \in G$ 都有 $1_{G} \cdot g = g \cdot 1_{G} = g$
3. （逆元）对于任意的 $g \in G$，总存在 $g^{-1} \in G$，使得 $g \cdot g^{-1} = g^{-1} \cdot g = 1_{G}$
满足交换律的群又称 **Abel 群**，我们一般把 Abel 群上的二元运算写成加法，把其上的单位元写成 $0$。

> **注** 在 2.4 节介绍群时，我们为方便读者理解，用的是 “单位元”。实际上当我们处理加法群的时候单位元显得并不合适，特别是在接下来的环和域中，会同时存在两个运算，加法和乘法。因此我们将群中的identity称为中性元，以作区分。

**环（ring）** 指的是一组资料 $(R, +, \times)$，其中 $R$ 是一个非空集合，其上有加法、乘法两个运算，满足下面的条件：
1. （Abel 群）$(R, +)$ 是一个 Abel 群，其上有中性元 $0_{R}$
2. （乘法）$R$ 上的乘法具有结合律，并有乘法单位元 $1_{R}$
3. （分配律）环上的加法和乘法满足分配律
如果环上的乘法也满足交换律，则称该环为**交换环**。

如果交换环 $R$ 上除了中性元 $0_{R}$ 其他的任意元素 $r$ 都存在逆（像实数、复数一样），则称其为一个 **域（field）**。

> **一些例子**
> * 整数集合 $\mathbb{Z}$ 关于加法形成一个群
> * 二维空间中的所有关于原点的旋转变换关于映射的复合构成一个群。其对应的所有矩阵常被记为 $SO(2)$
> * 所有实系数多项式 $\mathbb{R}[x]$ 关于多项式的加法和乘法构成一个环
> * 所有相同形状的方阵关于矩阵加法和乘法构成一个环，它不是交换环
> * 有理数集 $\mathbb{Q}$、实数集 $\mathbb{R}$ 和复数集 $\mathbb{C}$ 关于我们熟悉的乘法和加法构成域，这也是我们常说的**数域**

### 1.2.2 模和线性空间

假设 $R$ 是一个交换环，$M=(M, +)$ 是一个交换群，其中性元是 $0_{M}$。左 $R$-模包含一个这样的乘法： $\cdot: R \times M \rightarrow M$，满足下面的条件
1. 对任意 $r_{1}, r_{2} \in R$，$m \in M$，都有 $r_{1}\cdot(r_{2} \cdot m) = (r_{1}r_{2}) \cdot m$
2. 乘法分配律成立，即$$(r_{1}+r_{2}) \cdot m = r_{1} \cdot m + r_{2} \cdot m,\quad r\cdot(m_{1} + m_{2}) = r\cdot m_{1} + r\cdot m_{2}$$
3. $1_{R} \cdot m = m$
4. $0_{R} \cdot m = 0_{m}$

看起来是不是越来越像我们在线性代数课程中学到的那八条公理了？事实上，如果将上面的 $R$ 换成一个域 $k$，我们就称 $M$ 是一个 **$k$-线性空间**，并称 $M$ 里面的元素是**向量（vector）**，$R$里面的元素是 **标量（scalar）**。我们将线性空间中的域降级为模，一些好的结论依然成立。这里需要声明一个符号滥用：线性空间的标量乘法（数乘）和域中的乘法是不一样的，但为表述简便，在不引起歧义的情况下，我们将省略线性空间中的所以乘法（就像上一节中一样）。

> **一些例子**
> * Euclid空间。$\mathbb{R}^{n}$ 中的元素关于加法和 $\mathbb{R}$-数乘 形成一个线性空间
> * 连续函数空间。区间 $[a,b]$ 上的所有连续实值函数关于函数的加法和 $\mathbb{R}$ 乘法构成一个线性空间 （这是我们就无法将其画成向量了）
> * 线性映射。线性空间 $V$ 和 $W$ 之间的所有线性映射 $\text{Hom}(V, W)$ 关于线性映射的加法和数乘也构成线性空间

## 1.3 坐标变换

现在我们将视线转回 $\mathbb{R}^{n}$ 中的向量。我们有两个基 $\{ \boldsymbol{e}_{i}\}_{i=1}^{n}$ 和 $\{ \tilde{\boldsymbol{e}}_{i}\}_{i=1}^{n}$，对于 $\mathbb{R}^{n}$ 上的任一向量 $\boldsymbol{v}$，我们都可以将其用这两个基表出

$$
\begin{align}
\boldsymbol{v} &= v_{1} \boldsymbol{e}_{1} + v_{2} \boldsymbol{e}_{2} + \cdots + v_{n} \boldsymbol{e}_{n} \tag{11a}\\
\boldsymbol{v} &= \tilde{ v } _{1} \tilde{ \boldsymbol{e} } _{1} + \tilde{ v } _{2} \tilde{ \boldsymbol{e} } _{2} + \cdots + \tilde{ v } _{n} \tilde{ \boldsymbol{e} } _{n} \tag{11b}\\
\end{align}
$$

回忆基向量之间的变换，我们有

$$
\boxed{\begin{align}
{\color{red} \boldsymbol{e}_{i} }  &= \sum\limits_{j=1}^{n} b_{j, i} {\color{blue} \tilde{\boldsymbol{e}}_{j} }\\
{\color{blue} \tilde{\boldsymbol{e}}_{j} }  &= \sum\limits_{k=1}^{n} f_{k, j} {\color{red} \boldsymbol{e}_{k} }
\end{align}}
$$

将其代入 (11a)

$$
\begin{align}
\boldsymbol{v} &= v_{1} \boldsymbol{e}_{1} + v_{2} \boldsymbol{e}_{2} + \cdots + v_{n} \boldsymbol{e}_{n}\\
&= \sum\limits_{i=1}^{n} v_{i} {\color{red} \boldsymbol{e}_{i} }\\
&= \sum\limits_{i=1}^{n} v_{i} \left( \sum\limits_{j=1}^{n} b_{j, i} {\color{blue} \tilde{\boldsymbol{e}}_{j} } \right)\\
&= \sum\limits_{j=1}^{n} \left( \sum\limits_{i=1}^{n} b_{j,i} \cdot v_{i} \right) {\color{blue} \tilde{\boldsymbol{e}}_{j} }
\end{align}\tag{12}
$$

我们也可以相应地对 (11b) 做类似的替换，然后相互对照系数，最后得到下面的结果，并将其与基变换的法则进行比较：

$$
\mathop{\boxed{
\begin{align}
{\color{red} \tilde{ v }_{i} } = \sum\limits_{j=1}^{n} b_{i,j} {\color{blue} v_{j} } \\
{\color{blue} v_{i} } = \sum\limits_{j=1}^{n} f_{i,j} {\color{red} \tilde{ v }_{j} }
\end{align}
}}\limits_{{\color{red} \text{Contra} } \text{variant}} 
\quad 
\mathop{\boxed{\begin{align}
{\color{red} \boldsymbol{e}_{i} }  &= \sum\limits_{j=1}^{n} b_{j, i} {\color{blue} \tilde{\boldsymbol{e}}_{j} }\\
{\color{blue} \tilde{\boldsymbol{e}}_{j} }  &= \sum\limits_{k=1}^{n} f_{k, j} {\color{red} \boldsymbol{e}_{k} }
\end{align}}}\limits_{{\color{red} \text{Co} } \text{variant}} 
$$

读者不难发现，基变换和坐标变换的模式是反着来的。我们将坐标变换的性质称为 **反变的（<font color="red">contra</font>-variant）**，而将基变换的性质称为 **协变的（<font color="red">co</font>-variant）**。在后文中，我们对像向量坐标这样的反变的对象，我们将下标挪到上面。对于这个初看起来有些令人匪夷所思的性质，我们可以这样记忆：对于一个二维向量，如果将其伸长一倍，其坐标值也会增加一倍；如果将两个基向量伸长一倍，原向量的坐标值就会变为之前的二分之一。我们也可以考虑旋转：如果我们将两个基向量都逆时针旋转某角度，那么固定两个基向量的视角，看起来就像原来的向量顺时针旋转了相同的角度。

## 1.4 余向量及其分量的变换

我们在线性代数中常接触的两类向量被称为行向量和列向量。<font color="red">千万不要简单的认为余向量就是将列向量转置得到的行向量，这句话只有当你使用标准正交基时才是对的。</font>事实上，我们可以将余向量看做吃进一个向量的线性映射：$\alpha: V \rightarrow \mathbb{R}$。这在我们熟知的行列向量的上下文中是十分自然的：

$$
\begin{bmatrix}
2 & 1
\end{bmatrix}\left( 
\begin{bmatrix}
3 \\ -4
\end{bmatrix}
\right) = \begin{bmatrix}
2 & 1
\end{bmatrix}
\begin{bmatrix}
3 \\ -4
\end{bmatrix}
= 2 \cdot 3 + 1 \cdot (-4) = 2.
$$

我们说余向量是线性映射，具体说的是满足下面的条件

$$
\alpha(n\boldsymbol{v} + m \boldsymbol{w}) = n \alpha(\boldsymbol{v}) + m \alpha(\boldsymbol{w}).
$$

这条性质被称为**线性性（lineararity）**。对作用于二维Euclid空间的余向量，我们可以像画等高线一样将其画出来：

<center>
<img src="./ch2/attachments/covector_acts_on_vector.png" alt="余向量作用在向量上" style="zoom:60%;">
</center>
<center>图2 余向量作用在向量上</center>

想知道这个余向量作用在任意向量上的值是多少，只要看其跨越了多少等高线即可（不严谨的说法）。像一般的函数一样，余向量也可以加减，数乘。事实上对于作用于线性空间 $V$ 的余向量，它们的家位于一个被称为是 **对偶线性空间** 的地方，它被称为 $V^{*}$（也有书将其写为 $V^{\lor}$），对偶线性空间的几条基本性质和线性空间类似，这里不再赘述。

现在我们考虑余向量的变换。我们已经知道，线性空间的元素可以写成基向量的线性组合，那么构成余向量的基本部件是什么？以 $n$ 维线性空间为例子，并引入这样的余向量：

$$
\epsilon^{i} \in V^{*}, \epsilon^{i}(\boldsymbol{e}_{j}) = \delta_{i,j} \tag{13}
$$

这样就可以提取出余向量的各个“分量”了。我们接着拿一个任意的向量来实验：

$$
\begin{align}
\alpha(\boldsymbol{v}) &= \alpha\left( \sum\limits_{i=1}^{n} v^{i}\boldsymbol{e}_{i} \right) & \text{展开成基向量的线性组合}\\
&= \sum\limits_{i=1}^{n} v^{i}\alpha(\boldsymbol{e}_{i}) & \text{余向量的线性性}\\
&= \sum\limits_{i=1}^{n} \epsilon^{i}(v)\alpha(\boldsymbol{e}_{i}) & \text{(13)中的定义}\\
&= \sum\limits_{i=1}^{n} \alpha_{i}\epsilon^{i}(v) & \text{将}\alpha(\boldsymbol{e}_{i})\text{写为}\alpha_{i}
\end{align}
$$

这就得到了余向量的一个展开，上文中的 $\epsilon^{i}$ 称为 **对偶基（dual basis）**。读者可能发现，对偶基的标号是上标，而余向量的标号是下标。我们接下来就来证明余向量的基变换是反变的，坐标变换是协变的。假设对偶空间 $V^{*}$ 中有两个对偶基 $\{ \epsilon^{j} \}_{j=1}^{n}$ 和 $\{ \tilde{ \epsilon }^{i} \}_{i=1}^{n}$，对于每个对偶基 $\tilde{\epsilon}^{i}$ ，我们可以将其写成前一个对偶基中基向量的线性组合：

$$
\tilde{\epsilon}^{i} = \sum\limits_{j=1}^{n} Q_{i, j} \epsilon^{j}
$$

我们将 $\tilde{\epsilon}^{i}$ 作用在 $V$ 的一个基向量 $\tilde{\boldsymbol{e}}_{k}$ 上：

$$
\begin{align}
{\color{red} \tilde{\epsilon}^{i} } (\tilde{\boldsymbol{e}}_{k}) = \delta_{i,k} &= \sum\limits_{j=1}^{n} Q_{i, j} {\color{blue} \epsilon^{j} } ({\color{red} \tilde{\boldsymbol{e}}_{k} } )\\
&= \sum\limits_{j=1}^{n} Q_{i, j} {\color{blue} \epsilon^{j} } \left( \sum\limits_{l=1}^{n} f_{l,k} {\color{blue} \boldsymbol{e}_{l} }  \right)\\
&= \sum\limits_{l=1}^{n} \left[\sum\limits_{j=1}^{n} Q_{i, j} f_{l,k} {\color{blue} {\color{blue} \epsilon^{j} }(\boldsymbol{e}_{l}) }  \right]\\
&= \sum\limits_{l=1}^{n} \left[\sum\limits_{j=1}^{n} Q_{i, j} f_{l,k} \delta_{j,l}  \right]\\
&= \sum\limits_{j=1}^{n} Q_{i, j} f_{j,k} \\
\end{align}
$$

这就说明了由 $\{ \epsilon^{j} \}_{j=1}^{n}$ 变换到 $\{ \tilde{ \epsilon }^{i} \}_{i=1}^{n}$ 的矩阵是 $F$ 的逆矩阵，也就是 $B$。对偶地，读者也可以自行验证 反过来变换的矩阵 $P$ 是 $B$ 的逆，也就是 $F$。这说明了对偶基是反变的。我们把向量拿来作比较，如下所示

$$
\mathop{\boxed{\begin{align}
{\color{red} \boldsymbol{e}_{i} }  &= \sum\limits_{j=1}^{n} b_{j, i} {\color{blue} \tilde{\boldsymbol{e}}_{j} }\\
{\color{blue} \tilde{\boldsymbol{e}}_{j} }  &= \sum\limits_{k=1}^{n} f_{k, j} {\color{red} \boldsymbol{e}_{k} }
\end{align}}}\limits_{{\color{red} \text{Co} } \text{variant}} \quad 
\mathop{\boxed{\begin{align}
{\color{red} \epsilon^{i} }  &= \sum\limits_{j=1}^{n} f_{i,j} {\color{blue} \tilde{\epsilon}^{j} }\\
{\color{blue} \tilde{\epsilon}^{j} }  &= \sum\limits_{k=1}^{n} b_{j,k} {\color{red} \epsilon^{k} }
\end{align}}}\limits_{{\color{red} \text{Contra} } \text{variant}} 
$$

类似地，也可以证明余向量的坐标是协变的，这里不再赘述。

## 1.5 线性映射和矩阵

矩阵是线性映射在某个基下的表示。几何直观上看，如果把一个带有网格线的平面经过线性映射，得到的结果一定符合下面三个要求，如下图所示
1. 网格线映射过后依然是直的
2. 间隔相等的网格线映射过后间隔还是相等（间隔的距离可能变化）
3. 原点映射过后还是原点

<center>
<img src="./ch2/attachments/linear_map.png" alt="线性映射的集合直观。第一个图是映射前的网格，后面三个图分别代表由三个不同映射作用后的网格" style="zoom:60%;">
</center>
<center>图3 线性映射的集合直观。第一个图是映射前的网格，后面三个图分别代表由三个不同映射作用后的网格</center>

抽象地讲，线性映射是线性的（废话！）其遵守的线性性我们在余向量时已有提及，这里不再赘述，而是展示通常的矩阵和矩阵乘法是怎么来的。假设有一个线性映射 $L: V \rightarrow V$，其中 $V$ 的维数是 $2$。由于线性空间的元素总能写成基向量的线性组合，又由线性映射的性质，我们只需知道线性映射把基向量打到了哪里，就知道任意向量经过线性映射后的结果。假设 $L$ 作用在基向量上的结果如下，我们就像这样将其写为矩阵：

$$
\begin{align}
L(\boldsymbol{e}_{1}) = {\color{red} L_{1}^{1} }  \boldsymbol{e}_{1} + {\color{red} L_{1}^{2}} \boldsymbol{e}_{2}\\
L(\boldsymbol{e}_{2}) = {\color{blue} L_{2}^{1}} \boldsymbol{e}_{1} + {\color{blue} L_{2}^{2}} \boldsymbol{e}_{2}
\end{align} \quad L = \begin{bmatrix}
{\color{red} L_{1}^{1} } & {\color{blue} L_{2}^{1}}\\{\color{red} L_{1}^{2}} & {\color{blue} L_{2}^{2}}
\end{bmatrix}\tag{14}
$$

我们现在随便将一个向量 $\boldsymbol{v}$ 送进去看看：

$$
\begin{align}
L(\boldsymbol{v}) &= L(v^{1}\boldsymbol{e}_{1} + v^{2}\boldsymbol{e}_{2})\\
&= v^{1}L(\boldsymbol{e}_{1}) + v^2L(\boldsymbol{e}_{2}) & \text{线性性}\\
&= v^{1}\big( L_{1}^{1} \boldsymbol{e}_{1} + L_{1}^{2} \boldsymbol{e}_{2} \big) + v^{2} \big( L_{2}^{1} \boldsymbol{e}_{1} + L_{2}^{2} \boldsymbol{e}_{2} \big)  & \text{代入 (14)}\\
&= \big( v^{1}L_{1}^{1} + v^{2}L_{2}^{1} \big) \boldsymbol{e}_{1} + (v^{1}L_{1}^{2} + v^{2}L_{2}^{2})\boldsymbol{e}_{2} 
\end{align}
$$

这就定义了矩阵对列向量的乘法。矩阵对矩阵的乘法也是类似的，其本质是线性映射的复合。读者可以通过线性映射的性质自行推导矩阵乘法公式。

## 1.6 线性映射在不同基下表示的变换

从上一节读者可以看到，线性映射的矩阵与其采用的基是紧密相关的。我们现在看看如果换成一个其他的基，线性映射的矩阵会有什么变化。考虑 $n$ 维线性空间到它自己的线性映射 $L$ ，我们将基 $\{ \boldsymbol{e}_{j} \}_{j=1}^{n}$ 下的矩阵写为 $L$，在基 $\{ \tilde{\boldsymbol{e}}_{j} \}_{j=1}^{n}$ 下的矩阵写为 $\tilde{L}$。我们将 $L$ 作用在基向量 $\tilde{\boldsymbol{e}}_{i}$ 上，有

$$
\begin{align}
\sum\limits_{q=1}^{n} \tilde{ L }_{i}^{q} \tilde{\boldsymbol{e}}_{q} =L(\tilde{\boldsymbol{e}}_{i}) &= L\left( \sum\limits_{j=1}^{n} F_{i}^{j}\boldsymbol{e}_{j} \right) \\
&= \sum\limits_{j=1}^{n} F_{i}^{j} L\left(  \boldsymbol{e}_{j} \right)\\
&= \sum\limits_{j=1}^{n}  F_{i}^{j} \left( \sum\limits_{k=1}^{n} L_{j}^{k} \boldsymbol{e}_{k} \right)\\
&= \sum\limits_{j=1}^{n}  F_{i}^{j} \left[ \sum\limits_{k=1}^{n} L_{j}^{k} \left( \sum\limits_{l=1}^{n} B_{k}^{l} \tilde{\boldsymbol{e}}_{l} \right) \right]\\
&= \sum\limits_{l=1}^{n} \left[ \sum\limits_{j=1}^{n}   \sum\limits_{k=1}^{n} B_{k}^{l}L_{j}^{k}F_{i}^{j} \right] \tilde{\boldsymbol{e}}_{l}\\
\end{align}
$$

其中的 $F_{i}^{j}$，$B_{k}^{l}$ 是之前提到的正向变换矩阵和逆向变换矩阵。可见 $\tilde{L} = BLF$。从线性映射的角度也好理解。对于 $\{ \tilde{\boldsymbol{e}}_{j} \}_{j=1}^{n}$ 下的向量，先用 $F$ 将其坐标转换为基 $\{ {\boldsymbol{e}}_{j} \}_{j=1}^{n}$ 下的坐标，然后在这个基下做线性映射 $L$，然后再将 $\{ {\boldsymbol{e}}_{j} \}_{j=1}^{n}$ 下的像用 $B$ 转换回 $\{ \tilde{\boldsymbol{e}}_{j} \}_{j=1}^{n}$ 下的坐标，如下图所示（注意这里的作用顺序是 $F$，$L$，$B$，而不是反过来！）。

<center>
<img src="./ch2/attachments/linear_transform.png" alt="余向量作用在向量上" style="zoom:60%;">
</center>
<center>图4 线性映射在不同基下表示的转换</center>

相应地，我们也可以得到 $L = F \tilde{L} B$（只要将上图的两个竖直箭头反向即可）。

讲了这么多，这和张量有什么关系呢？余向量的基变换是反变的，我们将其称为一种 $(1,0)$-张量；向量是协变的，我们将其称为一种 $(0,1)$-张量。而线性变换在基变换下既用到了正向变换又用到了反向变换，我们将其称为一种 $(1,1)$-张量。

# 2 张量一瞥
## 2.1 度量张量
### 2.1.1 向量的长度

我们在中学时学过勾股定理，并用它来衡量一个向量的长度。但其限制是直角三角形，在线性空间基不是标准正交基时不再成立。事实上，向量的长度 $\|v\|$ 由下式给出

$$
\|v\|^{2} = \left\langle v, v \right\rangle 
$$

其中 $\left\langle \cdot, \cdot \right\rangle$ 是内积。对于一般的向量，我们可以写出它的平方长度：

$$
\|v\| = (v^{1})^{2} \left\langle e_{1}, e_{1}\right\rangle  + 2v^{1}v^{2} \left\langle e_{1}, e_{2} \right\rangle + (v^{2})^{2} \left\langle e_{2}, e_{2}\right\rangle
$$

也可以写成矩阵形式：

$$
\|v\| = \begin{bmatrix}
v^{1} & v^{2}
\end{bmatrix} \underbrace{ \begin{bmatrix}
\left\langle e_{1}, e_{1} \right\rangle & \left\langle e_{1}, e_{2} \right\rangle \\
\left\langle e_{2}, e_{1} \right\rangle & \left\langle e_{2}, e_{2} \right\rangle \\
\end{bmatrix} }_{ g_{e_{i}} } \begin{bmatrix}
v^{1}\\v^{2}
\end{bmatrix}
$$

其中 $g_{e_{i}}$ 就成为在基 $\boldsymbol{e}_{i}$ 下的度量张量。这是我们可以立即看出勾股定理在直角三角形时恰好成立的原因是不同的标准正交基向量的内积为零，因为此时度量张量是单位矩阵 $I$，而这恰好给出了勾股定理。当基变化时，度量张量会怎么变化呢？请看：将 $g_{e_{i}}$ 简记为 $g$，把新的基 $\{ \tilde{e}_{j} \}$ 下的度量张量记作 $\tilde{g}$，则其分量 $\tilde{g}_{i,j}$ 是两个新的基向量的内积

$$
\begin{align}
\tilde{g}_{i,j} &= \left\langle \tilde{e}_{i}, \tilde{e}_{j} \right\rangle\\
&= \left\langle \sum\limits_{k}^{n} F_{i}^{k} {e}_{k}, \sum\limits_{l=1}^{n} F_{j}^{l} {e}_{l} \right\rangle\\
&= \sum\limits_{k=1}^{n}\sum\limits_{l=1}^{n} F_{i}^{k}F_{j}^{l} \left\langle  {e}_{k},   {e}_{l} \right\rangle\\
&= \sum\limits_{k=1}^{n}\sum\limits_{l=1}^{n} F_{i}^{k}F_{j}^{l} g_{k,l}\\
\end{align}
$$

因此我们可以看到基发生变化后，度量张量要做两次正向变换得到新的基下的度量张量，也即两个协变变换。因此我们称其为一种 $(0,2)$-张量。

此时大家应该理解了张量的第二个定义：张量是在坐标变换变换下**保持不变**的数学对象，其分量在坐标变换时以一种特殊且可预测的方式变化。一般的张量的变换可以写成

$$
\tilde{T}_{x,y,z,\dots}^{a,b,c,\dots} = \Big( B_{i}^{a}B_{j}^{b}B_{k}^{c} \cdots  \Big) T_{r,s,t \dots}^{i,j,k, \dots} \Big( F_{x}^{r}F_{y}^{s}F_{z}^{t} \cdots  \Big) 
$$

其中 $T$ 的下标是协变分量，其变换要使用正向变换 $F$，而上标是反变分量，其变换要使用逆向变换 $B$。如果一个张量有 $m$ 个反变分量，$n$ 个协变分量，我们就称其为一个 $(m,n)$-张量。

## 2.2 双线性型

读者可能已注意到，上文中的内积或度量张量单独对输入的两个向量都是线性的。这样的函数有一个名字：**双线性型（bilinear form）**。线性空间 $V$ 上的双线性型 $\mathcal{B}: V \times V \rightarrow \mathbb{R}$ 满足固定一个分量，对另一个分量线性。具体而言就是
1. $a \mathcal{B}(v, w) = \mathcal{B}(av, w) = \mathcal{B}(v, aw)$
2. $\mathcal{B}(v+u, w) = \mathcal{B}(v, w) + \mathcal{B}(u, w)$
3. $\mathcal{B}(v, w+u) = \mathcal{B}(v, w) + \mathcal{B}(v, u)$

上文中提到的余向量也被称为**线性型（linear form）**。度量张量是一个特殊的双线性型，交换度量张量的两个输入，输出的值不变。一般的双线性型没有这样的良好性质。

## 2.4 线性映射由向量和余向量构成

这看起来很离谱。但不要着急，我们做下面的事情：对于矩阵 $A$ 我们总可以拆成这样的线性组合：

$$
A = \sum\limits_{i=1}^{n} \sum\limits_{j=1}^{m} A_{i}^{j} E_{i, j}.
$$

其中 $A_{i. j}$ 是矩阵在 $i$ 行 $j$ 列处的分量，$E_{i,j}$ 是除了 $i$ 行 $j$ 列其他地方都为零的矩阵。由基本的矩阵乘法知识我们有 $E_{i,j} = e_{i} e_{j}^{\top}$。所以上式写成余向量的形式就是

$$
A = \sum\limits_{i=1}^{n} \sum\limits_{j=1}^{m} A_{i}^{j} (e_{i}\epsilon^{j}) = \sum\limits_{j=1}^{n} \left( \sum\limits_{i=1}^{n} A_{i}^{j} e_{i} \right) \epsilon^{j}
$$

而最右边那一项说的就是我们通过线性映射作用在基上的结果构造矩阵的过程。回到主题，我们可以看见 $\{ e_{i}\epsilon_{j} \}$ 构成了线性映射空间的一个基。这样有什么好处呢？好处在于我们之前费很大功夫得出的线性映射的矩阵在基变换下的变化在用上这样的表示之后无比显然：

$$
\begin{align}
L =  \sum\limits_{k,l=1}^{n} L_{l}^{k} e_{k} \epsilon^{l} = \sum\limits_{k=1}^{n}L_{l}^{k}\left( \sum\limits_{i=1}^{n} B_{k}^{i}\tilde{e}_{i} \right)\left( \sum\limits_{j=1}^{n} F_{j}^{l}\tilde{\epsilon}^{j} \right) = \sum\limits_{i,j,k,l}^{} {\color{red} L_{l}^{k}B_{k}^{i}F_{j}^{l} } \tilde{e}_{i}\tilde{\epsilon}^{j}
\end{align}
$$

接下来我们将不将它们写成 $e_{i}\epsilon^{j}$，而将其写成 ${\color{red} e_{i} \otimes \epsilon^{j} }$。在用于矩阵这样的对象时（行列向量当然也是矩阵）$\otimes$ 被称为 **Kronecker积**。它把左边的矩阵搬到右边的每个元素的位置，然后数乘那个元素：

$$
\begin{bmatrix}
v^{1}\\v^{2}
\end{bmatrix} \otimes \begin{bmatrix}
\alpha_{1} & \alpha_{2}
\end{bmatrix} = \begin{bmatrix}
\alpha_{1}\cdot\begin{bmatrix}
v^{1}\\v^{2}
\end{bmatrix} &
\alpha_{2} \cdot \begin{bmatrix}
v^{1}\\v^{2}
\end{bmatrix} 
\end{bmatrix} = \begin{bmatrix}
\begin{bmatrix}
\alpha_{1}v^{1}\\\alpha_{1}v^{2}
\end{bmatrix} &
\begin{bmatrix}
\alpha_{1}v^{1}\\\alpha_{1}v^{2}
\end{bmatrix} 
\end{bmatrix}
$$

这看起来和将 Kronecker 积改成矩阵乘法得到的结果没什么区别，但这一思想对我们十分重要。

### 2.5 双线性型由余向量构成

双线性型吃进两个向量，吐出一个数。它也可以写成余向量的组合：

$$
\mathcal{B} = \sum\limits_{i,j} \mathcal{B}_{i,j} \epsilon^{i}  \epsilon^{j} = \sum\limits_{i,j} \mathcal{B}_{i,j} (\epsilon^{i} \otimes  \epsilon^{j}) 
$$

在使用这样的表示后双线性型在基变换下的变化也是显然的。我们也可以按照类似上一节的方法将行向量的 Kronecker 积结果写出来：

$$
\begin{bmatrix}
\alpha_{1} & \alpha_{2}
\end{bmatrix} \otimes \begin{bmatrix}
\beta_{1} & \beta_{2}
\end{bmatrix} = 
\begin{bmatrix}
\begin{bmatrix}
\alpha_{1}\beta_{1} & \alpha_{2}\beta_{1}
\end{bmatrix} & \begin{bmatrix}
\alpha_{1}\beta_{2} & \alpha_{2}\beta_{2}
\end{bmatrix}
\end{bmatrix}
$$

嗯......这看起来不太对劲，在我们的意识中度量张量似乎是个矩阵，为什么得到了这样的东西？回想度量张量的矩阵形式，它按道理应该吃两个向量，结果确实一个行向量和一个列向量。它之所以为一个矩阵是因为我们滥用了记号，让一个向量变成行向量（余向量），迫使度量张量看起来像个矩阵。如果我们就使用列向量的表示，那么上面那个怪怪的玩意儿其实更加合理：

$$
\begin{align}
\begin{bmatrix}
\begin{bmatrix}
\alpha_{1}\beta_{1} & \alpha_{2}\beta_{1}
\end{bmatrix} & \begin{bmatrix}
\alpha_{1}\beta_{2} & \alpha_{2}\beta_{2}
\end{bmatrix}
\end{bmatrix} \begin{bmatrix}
v^{1}\\v^{2}
\end{bmatrix} \begin{bmatrix}
w^{1}\\w^{2}
\end{bmatrix} &= 
\Big(
v^{1}\begin{bmatrix}
\alpha_{1}\beta_{1} & \alpha_{2}\beta_{1}
\end{bmatrix}
 +
v^{2}\begin{bmatrix}
\alpha_{1}\beta_{2} & \alpha_{2}\beta_{2}
\end{bmatrix}
\Big)
\begin{bmatrix}
w^{1}\\w^{2}
\end{bmatrix}\\
&= \begin{bmatrix}
v^{1}\alpha_{1}\beta_{1} + v^{2}\alpha_{1}\beta_{2} & v^{1}\alpha_{2}\beta_{1} + v^{2} \alpha_{2} \beta_{2}
\end{bmatrix}\begin{bmatrix}
w^{1}\\w^{2}
\end{bmatrix} \in \mathbb{R}.
\end{align}
$$

张量积和 Kronecker 共用一个符号，但含义不同。张量积作用于张量、线性空间和模，而 Kronecker 积作用于数组（如矩阵）。张量积具有多重线性，即
* $(\alpha v_{1}) \otimes v_{2} \otimes v_{3} \otimes \cdots = v_{1} \otimes (\alpha v_{2}) \otimes v_{3} \otimes \cdots = \cdots$
* $(v_{1}+w)\otimes v_{2}\otimes v_{3} \otimes \cdots = v_{1}\otimes v_{2}\otimes v_{3}\otimes \cdots + w \otimes v_{2} \otimes v_{3} \otimes \cdots$ （对其他位置的对象也一样）
当然张量积的多重线性性 Kronecker 也具备。

一般的张量也可以写成向量和余向量之间的张量积的线性组合。换句话说，形如 $a \otimes b$ 这样的对象是张量的基本组成部件。有了它们就可以轻易地得到任意形式的向量在基变换下的变化规律了。对于连个张量缩并，我们需要指出对那些分量缩并。例如 $Q_{j,k}^{i}$ 与 $D^{a,b}$ ，有多种缩并形式，如 $Q_{j, k}^{i}D^{j,k}$ 和 $Q_{j,k}^{i}D^{j,l}$，其中两个张量对应相同的上下标就是缩并求和掉的维度，在 `pytorch` 中可以使用 `einsum` 函数完成张量的缩并。

使用 Kronecker 积可以方便地将张量表示为数组，但容易使人迷失在数字的阵列中，而逐渐忘记该张量的性质。最后，线性空间的张量积这样一来也不难理解。两个线性空间 $V$ 和 $W$ 的张量积 $V \otimes W$ 里面包含了形如 $v \otimes w$ 的元素。其中 $v$ 和 $w$ 分别是 $V$ 和 $W$ 中的元素。





